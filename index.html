<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta property="og:type" content="website">
<meta property="og:title" content="WANG Yifan">
<meta property="og:url" content="http://alanwang93.github.io/index.html">
<meta property="og:site_name" content="WANG Yifan">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="WANG Yifan">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://alanwang93.github.io/"/>





  <title> WANG Yifan </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">WANG Yifan</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://alanwang93.github.io/2016/11/22/CS224d-notes-5-6/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="WANG Yifan">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="WANG Yifan">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="WANG Yifan" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/11/22/CS224d-notes-5-6/" itemprop="url">
                  CS224d notes (5,6)
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-11-22T00:00:01+01:00">
                2016-11-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Lecture-5"><a href="#Lecture-5" class="headerlink" title="Lecture 5"></a>Lecture 5</h2><h3 id="Max-margin-objective-function-window-classification"><a href="#Max-margin-objective-function-window-classification" class="headerlink" title="Max-margin objective function (window classification)"></a>Max-margin objective function (window classification)</h3><p>For a single window</p>
<script type="math/tex; mode=display">
J = max(0,1-s+s_c) \\
\text{where $s$ is the score of current sample and $s_c$ the score for corrupt(negative) sample}\\
\text{we hope $s$ is big and $s_c$ is small} \\
\text{when $s>s_c+1$, $J=0$, we can ignore the window, stop back propagation}</script><ul>
<li>It means that positive sample has a score +1 higher than negative sample<ul>
<li>xxx |&lt;=  1  =&gt;| ooo</li>
</ul>
</li>
<li>Advantage, stop back propagation when J becomes 0</li>
</ul>
<h4 id="Back-propagation-example"><a href="#Back-propagation-example" class="headerlink" title="Back propagation example"></a>Back propagation example</h4><script type="math/tex; mode=display">
J = max(0,1-s+s_c) \\
s = U^Tf(Wx+b) \\
s_c = U^Tf(Wx_c+b)</script><p>Compute the derivatives with respect to U, W, b, x</p>
<script type="math/tex; mode=display">
\frac{\partial s}{\partial U} =  \frac{\partial U^Ta}{\partial U} = a = f(Wx+b)</script><p>… a lot more</p>
<h2 id="Lecture-6"><a href="#Lecture-6" class="headerlink" title="Lecture 6"></a>Lecture 6</h2><h3 id="Neural-tips-amp-tricks"><a href="#Neural-tips-amp-tricks" class="headerlink" title="Neural tips &amp; tricks"></a>Neural tips &amp; tricks</h3><h4 id="Multi-task-learning-Weight-sharing"><a href="#Multi-task-learning-Weight-sharing" class="headerlink" title="Multi-task learning / Weight sharing"></a>Multi-task learning / Weight sharing</h4><p>see “NLP (almost) from scratch, Collobert et al. 2011”</p>
<h4 id="Genetal-Strategy-for-Successful-NNets"><a href="#Genetal-Strategy-for-Successful-NNets" class="headerlink" title="Genetal Strategy for Successful NNets"></a>Genetal Strategy for Successful NNets</h4><ol>
<li><p>Select appropriate network structure</p>
<ol>
<li><p>Single words, fixed window, sentence based, document level; bag of words, recursive vs. recurrent, CNN</p>
</li>
<li><p>Nonlinearity</p>
<ol>
<li><p>sigmoid (logistic): not good</p>
<script type="math/tex; mode=display">
f(z) = \frac{1}{1+exp(-z)} ~~~ f'(z) = f(z)(1-f(z))</script></li>
<li><p>tanh: bet in many models</p>
<script type="math/tex; mode=display">
f(z) = tanh(z) = \frac{e^z - e^{-z}}{e^z+e^{-z}} ~~~ f'(z) = 1-f(z)^2 \\
tanh(z) = 2logistic(2z)-1</script></li>
<li><p>ReLu (rectified linear): avoid <strong>gradient vanishing</strong></p>
<script type="math/tex; mode=display">
rect(z) = max(z,0)</script></li>
<li><p>Hard tanh</p>
</li>
<li><p>soft sign</p>
</li>
</ol>
</li>
</ol>
</li>
<li><p>Gradient checks &amp; model simplification (一步一步implement)</p>
</li>
<li><p>Parameter Initialization</p>
<ol>
<li><p>If z if great, then the derivative could be close to 0 (e.g. for tanh)</p>
</li>
<li><p><strong>hidden bias</strong> initialized to 0</p>
</li>
<li><p><strong>output bias</strong> initialized to optimal value if weights were 0 (e.g. mean target or inverse sigmoid of mean target)</p>
</li>
<li><p>x between -1 and 1</p>
</li>
<li><p><strong>Weights</strong> W initialized to so that z is small enough to be on a linear regime. Initialized to ~ Uniform(-r, r), r inversely proportional to fan-in (previos layer size) and fan-out (next laeyer size): </p>
<script type="math/tex; mode=display">
\sqrt{6/(\text{fan-in} + \text{fan-out})}</script><p>for tanh and 4 times bigger for sigmoid (see Glorot AISTATS 2010)</p>
</li>
<li><p>For ReLu, to avoid getting 0, we can initialize the bias in the positive part of the value</p>
</li>
</ol>
</li>
<li><p>Mini-batch SGD (SGD updates after only 1 example while mimi-batch after a batch)</p>
<script type="math/tex; mode=display">
\theta^{new} = \theta^{old} - \alpha \nabla_{\theta}J_{t:t+B}(\theta)</script><ol>
<li><p>size of batch: 20 to 1000</p>
</li>
<li><p>helps parallelizing</p>
</li>
<li><p><strong>Momentum</strong>: add a fraction v of previos update to current one, build up velocity in direction of consistent gradient</p>
<script type="math/tex; mode=display">
v = \mu v - \alpha \nabla_{\theta}J_t(\theta) \\
\theta^{new} = \theta^{old} + v</script><ul>
<li>v is initialized at 0</li>
<li>common μ = 0.9</li>
<li>momentum increased after some epochs (0.5 -&gt; 0.99)</li>
</ul>
</li>
<li><p><strong>Learning Rates</strong> </p>
<ol>
<li><p>reduce by 0.5 when validation error stops improving</p>
</li>
<li><p><strong>Adagrad</strong>: adaptive learning rate for each parameter</p>
<script type="math/tex; mode=display">
g_{t,i} = \frac{\partial}{\partial \theta^t_i}J_t(\theta)\\
\theta_{t-1,i} - \frac{\alpha}{\sqrt{\sum_{\tau=1}^t g_{\tau,i}^2}}g_{t,i}</script><p>Problem: with time going by, learning rate will be really small. Solution: reset the sum (or Adam)</p>
</li>
</ol>
</li>
<li><p>Regularize</p>
<ul>
<li>Reduce model size!</li>
<li>L1, L2 regularization</li>
<li>early stopping: use parameters that gave best validation error (keep wights for the last 50 iterations)</li>
<li><strong>Dropout</strong> (Hinton et al. 2012) <ul>
<li>Training time: at each instance of evaluation, randomly set 50% of the inputs to each neuron to 0.</li>
<li>Test time: halve(减半) the model weights (now twice as many)</li>
</ul>
</li>
</ul>
</li>
<li><p>Y. Bengio (2012), “Pratical Recommendations for Gradient-Based Training of Deep Architectures”</p>
<ol>
<li>Hyperparameter search: set a range and random search  </li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="Language-Models"><a href="#Language-Models" class="headerlink" title="Language Models"></a>Language Models</h3><p>   Definition: a language model computes a probability for a sequence of words, often conditioned on a window of n previos words</p>
<script type="math/tex; mode=display">
   P(w_1, \ldots,w_m) = \prod_{i=1}^m P(w_i|w_1,\ldots,w_{i-1}) \sim\prod_{i=1}^m P(w_i|w_{i-(n-1)},\ldots,w_{i-1})</script><ul>
<li><p>Original neural language model (A neural Probabilistic Language Model, Bengio et al. 2003)</p>
<ul>
<li><p>Problem: fixed window of context</p>
<p>To solve the problem:</p>
</li>
</ul>
</li>
</ul>
<h3 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h3><p>   Main idea: use the same set f W weights at all time steps</p>
<script type="math/tex; mode=display">
   h_t = \sigma(W^{(hh)}h_{t-1} + W^{(hx)}x[t]) \\
   \hat y_t = softmax(W^{(S)}h_t) \\
   \hat P(x_{t+1} = v_j|x_t,\ldots,x_1) = \hat y_{t,j }</script><ul>
<li><p>initialization</p>
<script type="math/tex; mode=display">
h_0 \in \mathbb{R}^{D_h} \text{ vecotr for the hidden layer at step 0}, D_h \text{dimension of hidden layer} \\
x[t] \text{ column vector of L at index [t] at step t} \\
W^{(hh)} \in \mathbb{R}^{D_h\times D_h} ~~~ W^{(hx)}\in \mathbb{R}^{D_h \times d} ~~~ W^{(S)} \in \mathbb{R}^{|V| \times D_h}</script></li>
<li><p>Training RNNs is hard: Vainishing or exploding gradient</p>
</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://alanwang93.github.io/2016/11/19/CS224d-notes-4/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="WANG Yifan">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="WANG Yifan">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="WANG Yifan" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/11/19/CS224d-notes-4/" itemprop="url">
                  CS224d notes (4)
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-11-19T21:57:15+01:00">
                2016-11-19
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Lecture-4-Word-Window-Classification-and-Neural-Networks"><a href="#Lecture-4-Word-Window-Classification-and-Neural-Networks" class="headerlink" title="Lecture 4 :Word Window Classification and Neural Networks"></a>Lecture 4 :Word Window Classification and Neural Networks</h2><ul>
<li><p>Cross Entropy loss function for softmax:</p>
</li>
<li><script type="math/tex; mode=display">
J(\theta) = \frac{1}{N}\sum_{i=1}^N - log(P(y_i|x)) \\
\text{where } \widehat y_{y_i} = softmax(f_{y_i}) =  P(y_i|x) = \frac{exp(W_{y_i}x)}{\sum_{c=1}^C exp(W_cx)} \\
\text{and $W_y$ is a row(?) of $W$ that corresponds to $y$}</script></li>
<li><p>In most cases, the square error <strong>will not</strong> give good results</p>
</li>
<li><p>General ML parameters:</p>
</li>
<li><script type="math/tex; mode=display">
\theta = \begin{matrix} 
\left[
  \begin{array}{c}
  W_{.1} \\
  \vdots \\
  W_{.d}
 \end{array}
\right]
\end{matrix} \in \mathbb{R}^{Cd} \text{ where $C$ is the number of classes and $d$ is the dimension of input vectors}</script></li>
<li><p>In Deep Learning, we learn both W and word vectors x (update some pre-trained word vectors)</p>
</li>
<li><p>​</p>
</li>
<li><script type="math/tex; mode=display">
\theta = \begin{matrix} 
\left[
  \begin{array}{c}
  W_{.1} \\
  \vdots \\
  W_{.d} \\
  W_{x_1} \\
  \vdots \\
  W_{x_V}
 \end{array}
\right]
\end{matrix} \in \mathbb{R}^{Cd + Vd}\\
\text{ where $C$ is the number of classes, V is the number of tokens,}\\ \text{ $d$ is the dimension of input vectors}</script></li>
<li><p>If the data set is very large, it might be helpful to update the vectors to the task, otherwise don’t do it.</p>
</li>
</ul>
<h3 id="Word-vector-notation"><a href="#Word-vector-notation" class="headerlink" title="Word vector notation"></a>Word vector notation</h3><ul>
<li><p>Lookup table: word vector matrix L</p>
</li>
<li><script type="math/tex; mode=display">
L = [x_1, x_2 \ldots x_{|V|}] ~~\} ~d \\
\text{Conceptually, to get the word vector(we don't do it): } x_i = Le_i</script></li>
</ul>
<h3 id="Window-classification-NER"><a href="#Window-classification-NER" class="headerlink" title="Window classification (NER)"></a>Window classification (NER)</h3><p>In the example, we use a window of 5 words to predict the class of the center word. </p>
<script type="math/tex; mode=display">
\text{Let x be } x = \begin{matrix} 
\left[
  \begin{array}{c}
  x_1 \\
  x_2 \\
  x_3 \\
  x_4 \\
  x_5 \\
 \end{array}
\right]
\end{matrix} \in \mathbb{R}^{5d} \text{ where $d$ is the dimension of input vectors} \\
f = f(x) = Wx \in \mathbb{R}^C \\
W \in \mathbb{R}^{C\times 5d}</script><p>The loss function is cross entropy error, and use softmax to predict. </p>
<ul>
<li><p>Update the concatenated word vectors</p>
<p>​</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial x} -log(softmax(f_y(x))) = \sum_{c=1}^C - \frac{\partial log(softmax(f_y(x))}{\partial f_c(x)} \frac{\partial f_c(x)}{\partial x}</script></li>
<li><p>Write it in vector as a gradient</p>
</li>
<li><script type="math/tex; mode=display">
  \frac{\partial}{\partial f} -log(softmax(f_y(x))) = 
  \begin{matrix}
  \left[
    \begin{array}{c}
  \widehat y_1 \\
  \vdots \\
  \widehat y_y - 1 \\
  \vdots \\
  \widehat y_C
  \end{array}
  \right]  = 
  \end{matrix}
  [\widehat y - t] = \delta \in \mathbb{R}^C</script></li>
<li><p>So we have a vectoriel version, notice that x is vecotr of the window</p>
</li>
<li><script type="math/tex; mode=display">
  \nabla_xJ =  \sum_{c=1}^C - \frac{\partial log(softmax(f_y(x))}{\partial f_c(x)} \frac{\partial f_c(x)}{\partial x} = \sum_{c=1}^C \delta_cW_c^T = W^T\delta \in R^{5d} \\
  \text{where } \delta_c = \delta_{window} \in R, ~ W_c \in R^{1\times 5d}
  \\ \delta_{window} =\begin{matrix}
  \left[
    \begin{array}{c}
  \nabla_{x_1} \\
  \nabla_{x_2} \\
  \nabla_{x_3} \\
  \nabla_{x_4} \\
  \nabla_{x_5} 
  \end{array}
  \right] 
  \end{matrix}</script></li>
<li><p>Add the gradient to the lookup table</p>
</li>
</ul>
<h5 id="Problems-of-Softmax"><a href="#Problems-of-Softmax" class="headerlink" title="Problems of Softmax"></a>Problems of Softmax</h5><ul>
<li>only give us a linear decision boundary</li>
<li>with little data can be a good regularizer but with much data is very limiting.</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://alanwang93.github.io/2016/11/19/CS224d-notes-1-2-3/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="WANG Yifan">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="WANG Yifan">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="WANG Yifan" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/11/19/CS224d-notes-1-2-3/" itemprop="url">
                  CS224d notes (1,2,3)
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-11-19T21:07:26+01:00">
                2016-11-19
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Lecture-2"><a href="#Lecture-2" class="headerlink" title="Lecture 2"></a>Lecture 2</h2><h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><ul>
<li><p>Objective function:</p>
</li>
<li><script type="math/tex; mode=display">
J(\theta) = \frac{1}{T}\sum_{t=1}^T\sum_{-m\leq j \leq m,j\neq 0}log(p(w_{t+j}|w_t))</script></li>
<li><p>With probabilities defined as:</p>
</li>
<li><script type="math/tex; mode=display">
p(o|c) = \frac{exp(u^T_o v_c)}{\sum_{w=1}^W exp(u^T_w v_c)} \\
\text{where } v_c \text{ as input vector for center word and } u \text{ as_o output vector (for tokens in the window), }\\ W \text{ is the number of tokens.}</script></li>
<li><p>Two vectors (as input and output) for each word, in the end average them or concatenate them to get the word vector</p>
</li>
<li><p>Derivative</p>
<ul>
<li>​<script type="math/tex; mode=display">
\frac{\partial x^Ta}{\partial x} = \frac{\partial a^Tx}{\partial x} = a \\
x^Ta = \sum_i x_ia_i \ \text{, so we have } \frac{\partial x^Ta}{\partial x_i} =   a_i</script></li>
</ul>
</li>
</ul>
<h2 id="Lecture-3"><a href="#Lecture-3" class="headerlink" title="Lecture 3"></a>Lecture 3</h2><h3 id="Word2Vec-1"><a href="#Word2Vec-1" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><ul>
<li><p>Parameters:</p>
</li>
<li><script type="math/tex; mode=display">
\theta = 
\begin{matrix}       %开始数学环境
\left[                 %左括号
  \begin{array}{c}   %该矩阵一共3列，每一列都居中放置
    v_1\\  %第一行元素
    \vdots \\
    v_V \\
    u_1 \\
    \vdots \\
    u_V
  \end{array}
\right]                 %右括号
\end{matrix}
\in \mathbb{R}^{2dV}   ~~~~\text{ where } V \text{ is the number of tokens and  } d \text{ the demension of word vectors.}</script></li>
<li><p>Optimization:</p>
<ul>
<li>Gradient Descente</li>
<li>Stochatic Gradient Descente<ul>
<li>sample some windows and calculate the gradient</li>
<li>Problem: In each window, only have at most 2c-1(?) words, one vector for input and 2m vectors for outputs, really sparse.<ul>
<li>solution: keep around hash for word vectors, or store it as 2 matrix U and V, each has columns of word vecotors and only update those that appear</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Initialization of parameters</p>
<ul>
<li>small numbers of some random distribution</li>
</ul>
</li>
</ul>
<h5 id="Skip-gram-model-and-negative-sampling"><a href="#Skip-gram-model-and-negative-sampling" class="headerlink" title="Skip-gram model and negative sampling"></a>Skip-gram model and negative sampling</h5><p>see “Distributed Representations of Words and Phrases and their Compositionality” (Mikolov et al. 2013)</p>
<ul>
<li>predict surrounding word vecotrs using the center word vector</li>
</ul>
<ul>
<li><p>Problems: normalization term in probability term is two computationally complex (run over all the word vectors).</p>
</li>
<li><p>Objective function: (negative sampling: randomly pick k tokens that don’t appear in the window)</p>
</li>
<li><script type="math/tex; mode=display">
J(\theta) = \frac{1}{T}\sum_{t=1}^T J_t(\theta) \\
\text{with } J_t(\theta) = log~ \sigma(u_o^Tv_c) + \sum_{i=1}^k \mathbb{E}_{j \sim P(\omega)}[log~\sigma(u^T_jv_c)] \\
\text{and } \sigma(x) = \frac{1}{1+e^{-x}}</script></li>
<li><p>The indexes follow the unigram distribution, and the power lowers the frequency of frequent tokens</p>
</li>
<li><script type="math/tex; mode=display">
j \sim U(w)^{\frac{3}{4}}</script></li>
</ul>
<h5 id="CBOW-model-Continous-Bag-Of-Words"><a href="#CBOW-model-Continous-Bag-Of-Words" class="headerlink" title="CBOW model (Continous Bag Of Words)"></a>CBOW model (Continous Bag Of Words)</h5><ul>
<li>Main idea: predict the center word with the sum of surrounding word vectors</li>
</ul>
<h5 id="Comparaison-to-other-models-like-LSA"><a href="#Comparaison-to-other-models-like-LSA" class="headerlink" title="Comparaison to other models like LSA"></a>Comparaison to other models like LSA</h5><h5 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h5><p>“GloVe: Global Vectors for Word Representation” </p>
<ul>
<li><p>Use the global co-occurence statistics, <strong>fast training</strong>, scalable, <strong>good performence with small corpus</strong> and small vectors</p>
</li>
<li><p>Obective function</p>
</li>
<li><script type="math/tex; mode=display">
J(\theta) = \frac{1}{2} \sum_{i,j=1}^W f(P_{ij})(u^T_iv_j - logP_{ij})^2 \\
\text{where } f(x) \text{ is like: } f = \left\{  
\begin{aligned}
 &x  \text{ if } ~ x <0.5\\ 
 &1 \text{ otherwise} \\
\end{aligned}
\right.</script></li>
</ul>
<h4 id="Evaluation-of-word-vector"><a href="#Evaluation-of-word-vector" class="headerlink" title="Evaluation of word vector"></a>Evaluation of word vector</h4><ul>
<li><p>Intrinsic </p>
<ul>
<li><p>word vector analogies</p>
<ul>
<li><p><a href="http://code.google.com/p/word2vec/source/browse/trunk/questions-words.txt" target="_blank" rel="external">http://code.google.com/p/word2vec/source/browse/trunk/questions-words.txt</a></p>
</li>
<li><p>a:b :: c:d? </p>
<script type="math/tex; mode=display">
d = arg\max_i \frac{(x_b-x_a+x_c)^Tx_i}{||x_b-x_a+x_c||}</script></li>
<li><p>What if the relation is not linear?</p>
</li>
</ul>
</li>
<li><p>Word similarity (based on human judgement)</p>
<ul>
<li>dataset: Wordsim353</li>
</ul>
</li>
</ul>
</li>
<li><p>Extrinsic </p>
</li>
</ul>
<h5 id="Do-we-need-to-train-word-vectors-on-task-corpus-or-on-general-corpus"><a href="#Do-we-need-to-train-word-vectors-on-task-corpus-or-on-general-corpus" class="headerlink" title="Do we need to train word vectors on task corpus or on general corpus?"></a>Do we need to train word vectors on task corpus or on general corpus?</h5><h4 id="Ambiguity"><a href="#Ambiguity" class="headerlink" title="Ambiguity"></a>Ambiguity</h4><p>Improving Word Representations Via Global Context And Multiple Word Prototypes (Huang et al. 2012)</p>
<p>A bad intrinsic evaluation result doesn’t result in a bad extrinsic evaluation result.</p>
<h4 id="Hyperparameters"><a href="#Hyperparameters" class="headerlink" title="Hyperparameters"></a>Hyperparameters</h4><ul>
<li>dimension of vectors: ~200 - 300</li>
<li>window size: 4 - 6</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://alanwang93.github.io/2016/10/30/Python-notes/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="WANG Yifan">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="WANG Yifan">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="WANG Yifan" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/30/Python-notes/" itemprop="url">
                  Python notes
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-10-30T22:15:24+01:00">
                2016-10-30
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ol>
<li>对Corpus (List of tokens)快速建立词频表</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">collections.Counter(words).most_common(vocabulary_size - <span class="number">1</span>)</div><div class="line"><span class="comment"># words: list of tokens</span></div><div class="line"><span class="comment"># collections.Counter(words): Counter(&#123;token: frequency ...&#125;)</span></div><div class="line"><span class="comment"># Counter.most_common(n): [(token, frequency)...] : n most frequent tokens</span></div></pre></td></tr></table></figure>
<ol>
<li>List 拼接</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">a + b</div><div class="line">a.extend(b) <span class="comment"># equals to a = a + b</span></div></pre></td></tr></table></figure>
<ol>
<li>创建 reverse dictionary</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))</div></pre></td></tr></table></figure>
<p>以上几段代码来自Udacity Deep Learning course assignment 5, 具体如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">vocabulary_size = <span class="number">50000</span></div><div class="line"><span class="comment"># words: list of tokens</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dataset</span><span class="params">(words)</span>:</span></div><div class="line">  <span class="comment"># UNK means "Unknown"</span></div><div class="line">  count = [[<span class="string">'UNK'</span>, <span class="number">-1</span>]]</div><div class="line">  count.extend(collections.Counter(words).most_common(vocabulary_size - <span class="number">1</span>)) </div><div class="line">  <span class="comment"># create token-freq pairs</span></div><div class="line">  dictionary = dict()</div><div class="line">  <span class="keyword">for</span> word, _ <span class="keyword">in</span> count:</div><div class="line">    dictionary[word] = len(dictionary) <span class="comment"># build index</span></div><div class="line">  data = list()</div><div class="line">  unk_count = <span class="number">0</span></div><div class="line">  <span class="keyword">for</span> word <span class="keyword">in</span> words:</div><div class="line">    <span class="keyword">if</span> word <span class="keyword">in</span> dictionary:</div><div class="line">      index = dictionary[word]</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">      index = <span class="number">0</span>  <span class="comment"># dictionary['UNK']</span></div><div class="line">      unk_count = unk_count + <span class="number">1</span></div><div class="line">    data.append(index) </div><div class="line">  count[<span class="number">0</span>][<span class="number">1</span>] = unk_count</div><div class="line">  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) </div><div class="line">  <span class="keyword">return</span> data, count, dictionary, reverse_dictionary</div><div class="line"><span class="comment"># data: documents as list of indexes</span></div><div class="line"><span class="comment"># count: List of (token, freq) pairs</span></div><div class="line"><span class="comment"># dictionary: &#123;token, index&#125;</span></div><div class="line"><span class="comment"># reverse_dictionary: &#123;index, token&#125;</span></div><div class="line">data, count, dictionary, reverse_dictionary = build_dataset(words)</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://alanwang93.github.io/2016/10/28/多层网络使用ReLu激活函数时Loss出现NaN的解决思路/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="WANG Yifan">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="WANG Yifan">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="WANG Yifan" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/28/多层网络使用ReLu激活函数时Loss出现NaN的解决思路/" itemprop="url">
                  多层网络使用ReLu激活函数时Loss出现NaN的解决思路
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-10-28T21:57:08+02:00">
                2016-10-28
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Udacity的Deep Learning课程的assignment 3要求用多层网络来完成noMINIST的分类问题。</p>
<p>针对loss function出现NaN的情况，有以下几种可能有用的办法：</p>
<ul>
<li><p>减小Learning rate</p>
</li>
<li><p>减小Batch size</p>
</li>
<li><p>用Gradient clipping，防止一些值过大，或者变为0</p>
</li>
<li><p>用tanh代替relu（不推荐）</p>
</li>
<li><p>使用L2 regularizartion</p>
</li>
<li><p>如果使用truncated_normalization，将stddev（默认为1）设置为一个较小的数</p>
</li>
<li><p>换一个loss function或者换一个optimizer</p>
<p>​</p>
</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://alanwang93.github.io/2016/10/27/zsh和iTerm2配置和一些shell命令/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="WANG Yifan">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="WANG Yifan">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="WANG Yifan" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/27/zsh和iTerm2配置和一些shell命令/" itemprop="url">
                  zsh和iTerm2配置和一些shell命令
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-10-27T20:53:39+02:00">
                2016-10-27
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Others/" itemprop="url" rel="index">
                    <span itemprop="name">Others</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="有用的一些shell命令"><a href="#有用的一些shell命令" class="headerlink" title="有用的一些shell命令"></a>有用的一些shell命令</h2><h5 id="用Finder打开当前文件夹（指定文件夹）"><a href="#用Finder打开当前文件夹（指定文件夹）" class="headerlink" title="用Finder打开当前文件夹（指定文件夹）"></a>用Finder打开当前文件夹（指定文件夹）</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">open . # 打开当前文件夹</div><div class="line">oepn &lt;dir&gt; # 打开指定文件夹</div></pre></td></tr></table></figure>
<h5 id="用指定（默认）程序打开目标文件"><a href="#用指定（默认）程序打开目标文件" class="headerlink" title="用指定（默认）程序打开目标文件"></a>用指定（默认）程序打开目标文件</h5><p>因为Vim用的不熟练，有时候会需要用文本编辑器打开文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">open -a &lt;editor&gt; &lt;file&gt; # 用&lt;editor&gt;打开&lt;file&gt;</div><div class="line">open -e &lt;file&gt; # 用文本编辑器打开文件</div><div class="line">open -t &lt;file&gt; # 用系统默认程序打开文件</div></pre></td></tr></table></figure>
<h2 id="zsh和iTerm2配置"><a href="#zsh和iTerm2配置" class="headerlink" title="zsh和iTerm2配置"></a>zsh和iTerm2配置</h2><h5 id="不同Tab不共享命令历史"><a href="#不同Tab不共享命令历史" class="headerlink" title="不同Tab不共享命令历史"></a>不同Tab不共享命令历史</h5><p>默认情况下iTerm2会在不同tab间共享命令历史，要取消共享，在<code>.zshrc</code>中加入以下代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">unsetopt inc_append_history</div><div class="line">unsetopt share_history</div></pre></td></tr></table></figure>
<h5 id="用Alias自定义命令"><a href="#用Alias自定义命令" class="headerlink" title="用Alias自定义命令"></a>用Alias自定义命令</h5>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="WANG Yifan" />
          <p class="site-author-name" itemprop="name">WANG Yifan</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">6</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">分类</span>
              
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">7</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WANG Yifan</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	




  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  


</body>
</html>
